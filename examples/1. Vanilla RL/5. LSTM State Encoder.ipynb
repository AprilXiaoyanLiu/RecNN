{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM state encoder\n",
    "P.S. This snippet uses library varsion of the learning function, you can see the visualization in the tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import gc\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# == recnn ==\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import recnn\n",
    "\n",
    "cuda = torch.device('cuda')\n",
    "# ---\n",
    "frame_size = 10\n",
    "batch_size = 25\n",
    "# --- \n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# https://drive.google.com/open?id=1kTyu05ZmtP2MA33J5hWdX8OyUYEDW4iI\n",
    "movie_embeddings_key_dict = pickle.load(open('../../data/embeddings/ml20_pca128.pkl', 'rb'))\n",
    "movies_embeddings_tensor, key_to_id, id_to_key = recnn.data.make_items_tensor(movie_embeddings_key_dict)\n",
    "# download ml20m dataset yourself\n",
    "ratings = pd.read_csv('../../data/ml-20m/ratings.csv')\n",
    "user_dict, users = recnn.data.prepare_dataset(ratings, key_to_id, frame_size)\n",
    "\n",
    "del ratings\n",
    "gc.collect()\n",
    "clear_output(True)\n",
    "clear_output(True)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    max_buf_size = 10000\n",
    "    buffer = ReplayBuffer(max_buf_size)\n",
    "\n",
    "    step = 0\n",
    "    while buffer.len() <= max_buf_size:\n",
    "        batch = next(iter(test_dataloader))\n",
    "        batch = [i.to(cuda) for i in batch]\n",
    "        items, ratings, sizes = batch\n",
    "        hidden = None\n",
    "        state = None\n",
    "        for t in range(int(sizes.min().item()) - 1):\n",
    "            action = items[:, t]\n",
    "            reward = ratings[:, t].unsqueeze(-1)\n",
    "            s = torch.cat([action, reward], 1).unsqueeze(0)\n",
    "            next_state, hidden = state_encoder(s, hidden) if hidden else state_encoder(s)\n",
    "            next_state = next_state.squeeze()\n",
    "\n",
    "            if np.random.random() > 0.95 and state is not None:\n",
    "                batch = [state, action, reward, next_state]\n",
    "                buffer.append(batch)\n",
    "\n",
    "            \n",
    "    loss = ddpg_update(buffer.get(), params, nets, optimizer,\n",
    "                       cuda, debugger, step=step, learn=False)\n",
    "    buffer.flush()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ddpg settings ===\n",
    "\n",
    "params = {\n",
    "    'gamma'      : 0.99,\n",
    "    'min_value'  : -10,\n",
    "    'max_value'  : 10,\n",
    "    'policy_step': 10,\n",
    "    'soft_tau'   : 0.001,\n",
    "    \n",
    "    'policy_lr'  : 1e-5,\n",
    "    'value_lr'   : 1e-5,\n",
    "    'actor_weight_init': 54e-2,\n",
    "    'critic_weight_init': 6e-1,\n",
    "}\n",
    "\n",
    "# === end ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "value_net  = recnn.models.Critic(256, 128, 256, params['critic_weight_init']).to(cuda)\n",
    "policy_net = recnn.models.Actor(256, 128, 256, params['actor_weight_init']).to(cuda)\n",
    "\n",
    "target_value_net = recnn.models.Critic(256, 128, 256).to(cuda)\n",
    "target_policy_net = recnn.models.Actor(256, 128, 256).to(cuda)\n",
    "\n",
    "state_encoder = nn.LSTM(129, 256, batch_first=True).to(cuda)\n",
    "\n",
    "target_policy_net.eval()\n",
    "target_value_net.eval()\n",
    "\n",
    "\n",
    "recnn.learning.soft_update(value_net, target_value_net, soft_tau=1.0)\n",
    "recnn.learning.soft_update(policy_net, target_policy_net, soft_tau=1.0)\n",
    "\n",
    "# optim.Adam can be replaced with RAdam\n",
    "pm = list(policy_net.parameters()) + list(state_encoder.parameters())\n",
    "value_optimizer = recnn.optim.RAdam(value_net.parameters(),\n",
    "                              lr=params['value_lr'], weight_decay=1e-2)\n",
    "policy_optimizer = recnn.optim.RAdam(pm, lr=params['policy_lr'] , weight_decay=1e-2)\n",
    "\n",
    "nets = {\n",
    "    'value_net': value_net,\n",
    "    'target_value_net': target_value_net,\n",
    "    'policy_net': policy_net,\n",
    "    'target_policy_net': target_policy_net,\n",
    "}\n",
    "\n",
    "optimizer = {\n",
    "    'policy_optimizer': policy_optimizer,\n",
    "    'value_optimizer':  value_optimizer\n",
    "}\n",
    "\n",
    "writer = SummaryWriter(log_dir='../../runs')\n",
    "debugger = recnn.Debugger(layout, run_tests, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4495554556e5419ebb9f2ff4e98ba860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = 1\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 25\n",
    "\n",
    "epoch_bar = tqdm(total=n_epochs)\n",
    "train_users = recnn.data.sort_users_itemwise(user_dict, users[:-5000])[2:]\n",
    "test_users = recnn.data.sort_users_itemwise(user_dict, users[-5000:])\n",
    "\n",
    "def prepare_batch_wrapper(batch):\n",
    "    batch = recnn.data.padder(batch)\n",
    "    batch = recnn.data.prepare_batch_dynamic_size(batch, movies_embeddings_tensor)\n",
    "    return batch\n",
    "    \n",
    "train_user_dataset = recnn.data.UserDataset(train_users, user_dict)\n",
    "test_user_dataset = recnn.data.UserDataset(test_users, user_dict)\n",
    "train_dataloader = DataLoader(train_user_dataset, batch_size=batch_size,\n",
    "                              num_workers=4,collate_fn=prepare_batch_wrapper)\n",
    "test_dataloader = DataLoader(test_user_dataset, batch_size=batch_size,\n",
    "                             num_workers=4,collate_fn=prepare_batch_wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_update(batch, params, nets, optimizer, device, debugger=False, learn=True, step=-1):\n",
    "    batch = [i.to(device) for i in batch]\n",
    "    state, action, reward, next_state = batch\n",
    "    # reward = reward.unsqueeze(1)\n",
    "\n",
    "    # --------------------------------------------------------#\n",
    "    # Value Learning\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_action = nets['target_policy_net'](next_state)\n",
    "        target_value = nets['target_value_net'](next_state, next_action.detach())\n",
    "        expected_value = reward + params['gamma'] * target_value\n",
    "        expected_value = torch.clamp(expected_value,\n",
    "                                     params['min_value'], params['max_value'])\n",
    "\n",
    "    value = nets['value_net'](state, action)\n",
    "\n",
    "    value_loss = torch.pow(value - expected_value.detach(), 2).mean()\n",
    "\n",
    "    if learn:\n",
    "        optimizer['value_optimizer'].zero_grad()\n",
    "        value_loss.backward(retain_graph=True)\n",
    "        optimizer['value_optimizer'].step()\n",
    "\n",
    "    elif not learn and debugger:\n",
    "            debugger.writer.add_figure('next_action',\n",
    "                                        plot.pairwise_distances_fig(next_action[:50]), step)\n",
    "            debugger.writer.add_histogram('value', value, step)\n",
    "            debugger.writer.add_histogram('target_value', target_value, step)\n",
    "            debugger.writer.add_histogram('expected_value', expected_value, step)\n",
    "\n",
    "    # --------------------------------------------------------#\n",
    "    # Policy learning\n",
    "\n",
    "    gen_action = nets['policy_net'](state)\n",
    "    policy_loss = -nets['value_net'](state, gen_action)\n",
    "\n",
    "    if not learn and debugger:\n",
    "        debugger.log_object('gen_action', gen_action, test=(not learn))\n",
    "        debugger.writer.add_histogram('policy_loss', policy_loss, step)\n",
    "        debugger.writer.add_figure('next_action',\n",
    "                          plot.pairwise_distances_fig(gen_action[:50]), step)\n",
    "    policy_loss = policy_loss.mean()\n",
    "\n",
    "    if learn and step % params['policy_step'] == 0:\n",
    "        optimizer['policy_optimizer'].zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm_(nets['policy_net'].parameters(), -1, 1)\n",
    "        optimizer['policy_optimizer'].step()\n",
    "\n",
    "        recnn.learning.soft_update(nets['value_net'], nets['target_value_net'], soft_tau=params['soft_tau'])\n",
    "        recnn.learning.soft_update(nets['policy_net'], nets['target_policy_net'], soft_tau=params['soft_tau'])\n",
    "\n",
    "    losses = {'value': value_loss.item(), 'policy': policy_loss.item(), 'step': step}\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = None\n",
    "        self.idx = 0\n",
    "        self.size = buffer_size\n",
    "        self.flush()\n",
    "        \n",
    "    def flush(self):\n",
    "        # state, action, reward, next_state\n",
    "        self.buffer = [torch.zeros(self.size, 256),\n",
    "                       torch.zeros(self.size, 128),\n",
    "                       torch.zeros(self.size, 1),\n",
    "                       torch.zeros(self.size, 256)]\n",
    "        self.idx = 0\n",
    "    \n",
    "    def append(self, batch):\n",
    "        \n",
    "        state, action, reward, next_state = batch\n",
    "        lower = self.idx\n",
    "        upper = state.size(0) + lower\n",
    "        self.buffer[0][lower:upper] = state\n",
    "        self.buffer[1][lower:upper] = action\n",
    "        self.buffer[2][lower:upper] = reward\n",
    "        self.buffer[3][lower:upper] = next_state\n",
    "        self.idx += upper\n",
    "        \n",
    "    def get(self):\n",
    "        return self.buffer\n",
    "    \n",
    "    def len(self):\n",
    "        return self.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3c18f09e0e4a3a83af308f31046197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5340), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e6124609ac13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_buf_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             loss = ddpg_update(buffer.get(), params, nets, optimizer,\n\u001b[0;32m---> 23\u001b[0;31m                                        cuda, debugger, step=step)\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mdebugger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3f34d03b49b8>\u001b[0m in \u001b[0;36mddpg_update\u001b[0;34m(batch, params, nets, optimizer, device, debugger, learn, step)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value_optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value_optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_buf_size = 100000\n",
    "buffer = ReplayBuffer(max_buf_size)\n",
    "\n",
    "step = 0\n",
    "for batch in tqdm(train_dataloader):\n",
    "    batch = [i.to(cuda) for i in batch]\n",
    "    items, ratings, sizes = batch\n",
    "    hidden = None\n",
    "    state = None\n",
    "    for t in range(int(sizes.min().item()) - 1):\n",
    "        action = items[:, t]\n",
    "        reward = ratings[:, t].unsqueeze(-1)\n",
    "        s = torch.cat([action, reward], 1).unsqueeze(0)\n",
    "        next_state, hidden = state_encoder(s, hidden) if hidden else state_encoder(s)\n",
    "        next_state = next_state.squeeze()\n",
    "        \n",
    "        if np.random.random() > 0.95 and state is not None:\n",
    "            batch = [state, action, reward, next_state]\n",
    "            buffer.append(batch)\n",
    "            \n",
    "        if buffer.len() >= max_buf_size:\n",
    "            loss = ddpg_update(buffer.get(), params, nets, optimizer,\n",
    "                                       cuda, debugger, step=step)\n",
    "            debugger.log_losses(loss)\n",
    "            step += 1\n",
    "            debugger.log_step(step)\n",
    "            buffer.flush()\n",
    "            \n",
    "            #if step % 100 == 0 and step > 0:\n",
    "            # debugger.test()\n",
    "            #clear_output(True)\n",
    "            #print(step)\n",
    "            #plotter.plot_loss()\n",
    "        \n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
