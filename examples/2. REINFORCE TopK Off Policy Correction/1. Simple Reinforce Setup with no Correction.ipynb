{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "The following code contains an implementation of the REINFORCE algorithm, **without Off Policy Correction, LSTM state encoder, and Noise Contrastive Estimation**. Look for these in other notebooks.\n",
    "\n",
    "Also, I am not google staff, and unlike the paper authors, I cannot have online feedback concerning the recommendations.\n",
    "\n",
    "**I use actor-critic for reward assigning.** In a real-world scenario that would be done through interactive user feedback, but here I use a neural network (critic) that aims to emulate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# == recnn ==\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import recnn\n",
    "\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "# ---\n",
    "frame_size = 10\n",
    "batch_size = 10\n",
    "n_epochs   = 100\n",
    "plot_every = 30\n",
    "step       = 0\n",
    "num_items    = 5000 # n items to recommend. Can be adjusted for your vram \n",
    "# --- \n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='grade3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will drop low freq items because it doesnt fit into my videocard vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataset(df, key_to_id, frame_size, env, sort_users=False):\n",
    "    \n",
    "    global num_items\n",
    "    \n",
    "    value_counts = df['movieId'].value_counts() \n",
    "    print('counted!')\n",
    "    \n",
    "    num_items = 5000\n",
    "    to_remove = df['movieId'].value_counts().sort_values()[:-num_items].index\n",
    "    to_keep = df['movieId'].value_counts().sort_values()[-num_items:].index\n",
    "    to_remove_indices = df[df['movieId'].isin(to_remove)].index\n",
    "    num_removed = len(to_remove)\n",
    "    \n",
    "    df.drop(to_remove_indices, inplace=True)\n",
    "    print('dropped!')\n",
    "    \n",
    "    print('before', env.embeddings.size(), len(env.movie_embeddings_key_dict))\n",
    "    for i in list(env.movie_embeddings_key_dict.keys()):\n",
    "        if i not in to_keep:\n",
    "            del env.movie_embeddings_key_dict[i]\n",
    "        \n",
    "    env.embeddings, env.key_to_id, env.id_to_key = recnn.data.utils.make_items_tensor(env.movie_embeddings_key_dict)\n",
    "    \n",
    "    print('after', env.embeddings.size(), len(env.movie_embeddings_key_dict))\n",
    "    print('embeddings automatically updated')\n",
    "    print('action space is reduced to {} - {} = {}'.format(num_items + num_removed, num_removed,\n",
    "                                                           num_items))\n",
    "    \n",
    "    return recnn.data.prepare_dataset(df, env.key_to_id, frame_size, sort_users=sort_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_contstate_discaction(batch, item_embeddings_tensor, frame_size, num_items, *args, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Embed Batch: continuous state discrete action\n",
    "    \"\"\"\n",
    "    \n",
    "    from recnn.data.utils import get_irsu\n",
    "    \n",
    "    items_t, ratings_t, sizes_t, users_t = get_irsu(batch)\n",
    "    items_emb = item_embeddings_tensor[items_t.long()]\n",
    "    b_size = ratings_t.size(0)\n",
    "\n",
    "    items = items_emb[:, :-1, :].view(b_size, -1)\n",
    "    next_items = items_emb[:, 1:, :].view(b_size, -1)\n",
    "    ratings = ratings_t[:, :-1]\n",
    "    next_ratings = ratings_t[:, 1:]\n",
    "\n",
    "    state = torch.cat([items, ratings], 1)\n",
    "    next_state = torch.cat([next_items, next_ratings], 1)\n",
    "    action = items_t[:, -1]\n",
    "    reward = ratings_t[:, -1]\n",
    "\n",
    "    done = torch.zeros(b_size)\n",
    "    done[torch.cumsum(sizes_t - frame_size, dim=0) - 1] = 1\n",
    "    \n",
    "    one_hot_action = torch.zeros(action.size(0), num_items)\n",
    "    one_hot_action.scatter_(1, action.view(-1,1), 1)\n",
    "\n",
    "    batch = {'state': state, 'action': one_hot_action, 'reward': reward, 'next_state': next_state, 'done': done,\n",
    "             'meta': {'users': users_t, 'sizes': sizes_t}}\n",
    "    return batch\n",
    "\n",
    "def embed_batch(batch, item_embeddings_tensor, *args, **kwargs):\n",
    "    return batch_contstate_discaction(batch, item_embeddings_tensor, frame_size=frame_size, num_items=num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counted!\n",
      "dropped!\n",
      "before torch.Size([27278, 128]) 27278\n",
      "after torch.Size([5000, 128]) 5000\n",
      "embeddings automatically updated\n",
      "action space is reduced to 26744 - 21744 = 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860d2596f2d4474196ef04afe422e37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18946308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39214de488874b528bba867f595c4b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18946308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cd076afa2f46d48f5bb2dd2feaba43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=138493), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# embeddgings: https://drive.google.com/open?id=1EQ_zXBR3DKpmJR3jBgLvt-xoOvArGMsL\n",
    "env = recnn.data.env.FrameEnv('../../data/embeddings/ml20_pca128.pkl',\n",
    "                              '../../data/ml-20m/ratings.csv', frame_size, batch_size,\n",
    "                              embed_batch=embed_batch, prepare_dataset=prepare_dataset,\n",
    "                              num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePolicy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, num_actions):\n",
    "        super(DiscretePolicy, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        action_scores = self.linear2(x)\n",
    "        return F.softmax(action_scores)\n",
    "    \n",
    "    \n",
    "    def select_action(self, state):\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        return action, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because I do not have a dynamic environment, I also will include a critic. If you have a real non static environment, you can do w/o citic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === reinforce settings ===\n",
    "\n",
    "params = {\n",
    "    'gamma'      : 0.99,\n",
    "    'min_value'  : -10,\n",
    "    'max_value'  : 10,\n",
    "    'policy_step': 10,\n",
    "    'soft_tau'   : 0.001,\n",
    "    \n",
    "    'policy_lr'  : 1e-5,\n",
    "    'value_lr'   : 1e-5,\n",
    "    'actor_weight_init': 54e-2,\n",
    "    'critic_weight_init': 6e-1,\n",
    "}\n",
    "\n",
    "# === end ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = {\n",
    "    'value_net': recnn.nn.Critic(1290, num_items, 2048, params['critic_weight_init']).to(cuda),\n",
    "    'target_value_net': recnn.nn.Critic(1290, num_items, 2048, params['actor_weight_init']).to(cuda).eval(),\n",
    "    \n",
    "    'policy_net':  DiscretePolicy(2048, 1290, num_items).to(cuda),\n",
    "    'target_policy_net': DiscretePolicy(2048, 1290, num_items).to(cuda).eval(),\n",
    "}\n",
    "\n",
    "\n",
    "# from good to bad: Ranger Radam Adam RMSprop\n",
    "optimizer = {\n",
    "    'value_optimizer': recnn.optim.Ranger(nets['value_net'].parameters(),\n",
    "                                          lr=params['value_lr'], weight_decay=1e-2),\n",
    "\n",
    "    'policy_optimizer': recnn.optim.Ranger(nets['policy_net'].parameters(),\n",
    "                                           lr=params['policy_lr'], weight_decay=1e-5)\n",
    "}\n",
    "\n",
    "\n",
    "loss = {\n",
    "    'test': {'value': [], 'policy': [], 'step': []},\n",
    "    'train': {'value': [], 'policy': [], 'step': []}\n",
    "    }\n",
    "\n",
    "debug = {}\n",
    "\n",
    "writer = SummaryWriter(log_dir='../../runs')\n",
    "plotter = recnn.utils.Plotter(loss, [['value', 'policy']],)\n",
    "device = cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_update(batch, learn=True):\n",
    "    \n",
    "    state, action, reward, next_state, done = recnn.data.get_base_batch(batch)\n",
    "    \n",
    "    # Value Learning\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_action = nets['target_policy_net'](next_state)\n",
    "        target_value   = nets['target_value_net'](next_state, next_action.detach())\n",
    "        expected_value = reward + (1.0 - done) * 0.99 * target_value\n",
    "        expected_value = torch.clamp(expected_value, -10, 10)\n",
    "\n",
    "    value = nets['value_net'](state, action)\n",
    "    value_loss = torch.pow(value - expected_value.detach(), 2).mean()\n",
    "    \n",
    "    if learn:\n",
    "        optimizer['value_optimizer'].zero_grad()\n",
    "        value_loss.backward()\n",
    "        optimizer['value_optimizer'].step()\n",
    "        \n",
    "    return value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE():\n",
    "    \n",
    "    @staticmethod\n",
    "    def reinforce(policy, returns, *args, **kwargs):\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        return policy_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def reinforce_with_correction():\n",
    "        raise NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    def call(policy, optimizer, algorithm=None):\n",
    "        \n",
    "        if algorithm is None:\n",
    "            algorithm  = REINFORCE.reinforce\n",
    "            \n",
    "        R = torch.tensor([0]).to(cuda)\n",
    "\n",
    "        returns = []\n",
    "        for r in policy.rewards[::-1]:\n",
    "            R = r + 0.99 * R\n",
    "            returns.insert(0, R)\n",
    "            \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 0.0001)\n",
    "\n",
    "        policy_loss = algorithm(policy, returns)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del policy.rewards[:]\n",
    "        del policy.saved_log_probs[:]\n",
    "\n",
    "        return policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_update(batch, params, nets, optimizer,\n",
    "                     writer=recnn.utils.DummyWriter(),\n",
    "                     device=torch.device('cpu'),\n",
    "                     debug=dict(), learn=False, step=-1):\n",
    "    \n",
    "    state, action, reward, next_state, done = recnn.data.get_base_batch(batch)\n",
    "    \n",
    "    predicted_action, predicted_probs = nets['policy_net'].select_action(state)\n",
    "    reward = nets['value_net'](state, predicted_probs).detach()\n",
    "    nets['policy_net'].rewards.append(reward.mean())\n",
    "    \n",
    "    value_loss = recnn.nn.value_update(batch, params, nets, optimizer,\n",
    "                     writer=writer,\n",
    "                     device=device,\n",
    "                     debug=debug, learn=True, step=step)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if step % params['policy_step'] == 0 and step > 0:\n",
    "        \n",
    "        policy_loss = REINFORCE.call(nets['policy_net'], optimizer['policy_optimizer'])\n",
    "        del nets['policy_net'].rewards[:]\n",
    "        del nets['policy_net'].saved_log_probs[:]\n",
    "        print('step: ', step, '| value:', value_loss.item(), '| policy', policy_loss.item())\n",
    "    \n",
    "        recnn.utils.soft_update(nets['value_net'], nets['target_value_net'], soft_tau=params['soft_tau'])\n",
    "        recnn.utils.soft_update(nets['policy_net'], nets['target_policy_net'], soft_tau=params['soft_tau'])\n",
    "\n",
    "        losses = {'value': value_loss.item(),\n",
    "                  'policy': policy_loss.item(),\n",
    "                  'step': step}\n",
    "\n",
    "        recnn.utils.write_losses(writer, losses, kind='train' if learn else 'test')\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56071c900bed44fabdd21333b8beaccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  10 | value: 50.69260025024414 | policy -54447.9140625\n",
      "step:  20 | value: 39.12702941894531 | policy -1322.011474609375\n",
      "step 30\n",
      "step:  30 | value: 32.70941162109375 | policy 15132.70703125\n",
      "step:  40 | value: 24.22550392150879 | policy 22501.21484375\n",
      "step:  50 | value: 23.58995246887207 | policy 13874.126953125\n",
      "step 60\n",
      "step:  60 | value: 18.77687644958496 | policy -18705.01953125\n",
      "step:  70 | value: 18.473474502563477 | policy -6204.1689453125\n",
      "step:  80 | value: 18.67662811279297 | policy -11902.884765625\n",
      "step 90\n",
      "step:  90 | value: 17.839340209960938 | policy -18216.49609375\n",
      "step:  100 | value: 17.994937896728516 | policy 12965.2548828125\n",
      "step:  110 | value: 18.77390480041504 | policy 897.503662109375\n",
      "step 120\n",
      "step:  120 | value: 17.465091705322266 | policy 18707.359375\n",
      "step:  130 | value: 17.891725540161133 | policy 12005.7431640625\n",
      "step:  140 | value: 17.914501190185547 | policy -7836.19140625\n",
      "step 150\n",
      "step:  150 | value: 17.158405303955078 | policy 16218.330078125\n",
      "step:  160 | value: 16.23208999633789 | policy -3695.52734375\n",
      "step:  170 | value: 17.82404327392578 | policy 2221.763671875\n",
      "step 180\n",
      "step:  180 | value: 18.12932777404785 | policy -3764.297607421875\n",
      "step:  190 | value: 17.468366622924805 | policy 2893.46728515625\n",
      "step:  200 | value: 17.103260040283203 | policy 6373.8984375\n",
      "step 210\n",
      "step:  210 | value: 17.368837356567383 | policy -12885.935546875\n",
      "step:  220 | value: 17.653905868530273 | policy -9428.880859375\n",
      "step:  230 | value: 16.153331756591797 | policy 4642.021484375\n",
      "step 240\n",
      "step:  240 | value: 16.819673538208008 | policy -5746.2998046875\n",
      "step:  250 | value: 18.465618133544922 | policy 9049.3818359375\n",
      "step:  260 | value: 15.655301094055176 | policy -13623.57421875\n",
      "step 270\n",
      "step:  270 | value: 17.89520263671875 | policy 5787.36083984375\n",
      "step:  280 | value: 16.25200843811035 | policy -4362.6376953125\n",
      "step:  290 | value: 16.92286491394043 | policy 4473.271484375\n",
      "step 300\n",
      "step:  300 | value: 16.460796356201172 | policy -22814.263671875\n",
      "step:  310 | value: 15.415589332580566 | policy -19769.9140625\n",
      "step:  320 | value: 14.958292961120605 | policy 1118.942138671875\n",
      "step 330\n",
      "step:  330 | value: 15.906512260437012 | policy -30808.0078125\n",
      "step:  340 | value: 14.363289833068848 | policy -2172.22607421875\n",
      "step:  350 | value: 16.03351593017578 | policy 8171.087890625\n",
      "step 360\n",
      "step:  360 | value: 15.674530029296875 | policy 8704.93359375\n",
      "step:  370 | value: 13.339921951293945 | policy 1290.847412109375\n",
      "step:  380 | value: 14.83121109008789 | policy 9057.5361328125\n",
      "step 390\n",
      "step:  390 | value: 14.23555850982666 | policy -1696.720703125\n",
      "step:  400 | value: 15.197053909301758 | policy -7529.0078125\n",
      "step:  410 | value: 15.000702857971191 | policy -12259.626953125\n",
      "step 420\n",
      "step:  420 | value: 15.035049438476562 | policy -13411.0205078125\n",
      "step:  430 | value: 13.835576057434082 | policy 3730.46337890625\n",
      "step:  440 | value: 13.25556755065918 | policy -12498.560546875\n",
      "step 450\n",
      "step:  450 | value: 15.148327827453613 | policy 1341.6436767578125\n",
      "step:  460 | value: 13.694725036621094 | policy 20598.60546875\n",
      "step:  470 | value: 14.348563194274902 | policy 9674.23828125\n",
      "step 480\n",
      "step:  480 | value: 14.962656021118164 | policy -14379.5693359375\n",
      "step:  490 | value: 13.942241668701172 | policy 1693.58349609375\n",
      "step:  500 | value: 13.168965339660645 | policy 3567.62158203125\n",
      "step 510\n",
      "step:  510 | value: 13.155750274658203 | policy 34129.6328125\n",
      "step:  520 | value: 14.517314910888672 | policy -11759.7255859375\n",
      "step:  530 | value: 11.428970336914062 | policy 7223.65966796875\n",
      "step 540\n",
      "step:  540 | value: 13.475593566894531 | policy -6694.4599609375\n",
      "step:  550 | value: 12.802573204040527 | policy 249.4800567626953\n",
      "step:  560 | value: 14.247233390808105 | policy 23471.1953125\n",
      "step 570\n",
      "step:  570 | value: 12.598285675048828 | policy -10623.896484375\n",
      "step:  580 | value: 12.600652694702148 | policy 9221.005859375\n",
      "step:  590 | value: 11.119125366210938 | policy 5502.599609375\n",
      "step 600\n",
      "step:  600 | value: 12.599915504455566 | policy 24308.79296875\n",
      "step:  610 | value: 12.741128921508789 | policy -4113.43212890625\n",
      "step:  620 | value: 11.847526550292969 | policy -15890.634765625\n",
      "step 630\n",
      "step:  630 | value: 11.295145988464355 | policy -2793.59521484375\n",
      "step:  640 | value: 12.418724060058594 | policy -23424.125\n",
      "step:  650 | value: 11.341926574707031 | policy -9449.33984375\n",
      "step 660\n",
      "step:  660 | value: 12.034204483032227 | policy 3677.0234375\n",
      "step:  670 | value: 11.971757888793945 | policy -4122.31591796875\n",
      "step:  680 | value: 10.479643821716309 | policy -3406.001220703125\n",
      "step 690\n",
      "step:  690 | value: 12.654623985290527 | policy -2376.279541015625\n",
      "step:  700 | value: 10.783466339111328 | policy 14773.5009765625\n",
      "step:  710 | value: 11.208087921142578 | policy 42719.4140625\n",
      "step 720\n",
      "step:  720 | value: 11.909844398498535 | policy 9736.296875\n",
      "step:  730 | value: 10.75381851196289 | policy -28929.18359375\n",
      "step:  740 | value: 10.812828063964844 | policy -57348.68359375\n",
      "step 750\n",
      "step:  750 | value: 10.872734069824219 | policy -35670.71875\n",
      "step:  760 | value: 11.678773880004883 | policy 24858.19921875\n",
      "step:  770 | value: 11.04393196105957 | policy -38788.7578125\n",
      "step 780\n",
      "step:  780 | value: 10.42300796508789 | policy -35321.9296875\n",
      "step:  790 | value: 10.647441864013672 | policy 5461.58203125\n",
      "step:  800 | value: 11.285014152526855 | policy -2400.382568359375\n",
      "step 810\n",
      "step:  810 | value: 10.311898231506348 | policy -19553.947265625\n",
      "step:  820 | value: 10.108929634094238 | policy 5572.73828125\n",
      "step:  830 | value: 9.226076126098633 | policy -8107.4140625\n",
      "step 840\n",
      "step:  840 | value: 10.727606773376465 | policy -2551.534423828125\n",
      "step:  850 | value: 10.238664627075195 | policy -13727.4365234375\n",
      "step:  860 | value: 10.137031555175781 | policy -4179.06396484375\n",
      "step 870\n",
      "step:  870 | value: 9.58932113647461 | policy -17369.525390625\n",
      "step:  880 | value: 10.278036117553711 | policy 3547.869873046875\n",
      "step:  890 | value: 9.953421592712402 | policy -16108.388671875\n",
      "step 900\n",
      "step:  900 | value: 9.716985702514648 | policy -9546.9013671875\n",
      "step:  910 | value: 10.3414888381958 | policy -15030.572265625\n",
      "step:  920 | value: 9.254600524902344 | policy 1686.09228515625\n",
      "step 930\n",
      "step:  930 | value: 10.181493759155273 | policy -22955.75390625\n",
      "step:  940 | value: 9.325057029724121 | policy -1617.5184326171875\n",
      "step:  950 | value: 8.058208465576172 | policy 9898.458984375\n",
      "step 960\n",
      "step:  960 | value: 9.062178611755371 | policy 4340.2294921875\n",
      "step:  970 | value: 9.71664047241211 | policy -7380.34033203125\n",
      "step:  980 | value: 9.009974479675293 | policy 21355.330078125\n",
      "step 990\n",
      "step:  990 | value: 8.229780197143555 | policy 30680.7109375\n",
      "step:  1000 | value: 9.006196022033691 | policy -33931.25\n",
      "step:  1010 | value: 8.009425163269043 | policy -997.9796142578125\n",
      "step 1020\n",
      "step:  1020 | value: 9.226424217224121 | policy 3630.55517578125\n",
      "step:  1030 | value: 8.755946159362793 | policy 7867.880859375\n",
      "step:  1040 | value: 8.734724044799805 | policy 24673.38671875\n",
      "step 1050\n",
      "step:  1050 | value: 8.867165565490723 | policy 22083.171875\n",
      "step:  1060 | value: 8.188582420349121 | policy -13809.09765625\n",
      "step:  1070 | value: 8.993847846984863 | policy 2092.811279296875\n",
      "step 1080\n",
      "step:  1080 | value: 8.24184513092041 | policy -17183.3828125\n",
      "step:  1090 | value: 8.440699577331543 | policy -30557.875\n",
      "step:  1100 | value: 8.538538932800293 | policy 31178.609375\n",
      "step 1110\n",
      "step:  1110 | value: 8.157173156738281 | policy 10810.8349609375\n",
      "step:  1120 | value: 8.100311279296875 | policy -9603.9990234375\n",
      "step:  1130 | value: 8.313217163085938 | policy -15826.23046875\n",
      "step 1140\n",
      "step:  1140 | value: 7.857426643371582 | policy -3945.106689453125\n",
      "step:  1150 | value: 8.17651081085205 | policy -2805.7529296875\n",
      "step:  1160 | value: 7.910145282745361 | policy -15681.5361328125\n",
      "step 1170\n",
      "step:  1170 | value: 7.884614944458008 | policy 15060.4580078125\n",
      "step:  1180 | value: 7.313398838043213 | policy 4282.0771484375\n",
      "step:  1190 | value: 7.108511447906494 | policy 20649.791015625\n",
      "step 1200\n",
      "step:  1200 | value: 7.425937175750732 | policy -21803.59375\n",
      "step:  1210 | value: 7.11858606338501 | policy -10456.63671875\n",
      "step:  1220 | value: 7.538021564483643 | policy -12314.34765625\n",
      "step 1230\n",
      "step:  1230 | value: 7.312631607055664 | policy -4209.8701171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  1240 | value: 7.234259605407715 | policy -8767.65234375\n",
      "step:  1250 | value: 7.516167163848877 | policy -1342.419189453125\n",
      "step 1260\n",
      "step:  1260 | value: 7.649448871612549 | policy 2115.92431640625\n",
      "step:  1270 | value: 7.189054489135742 | policy 11169.875\n",
      "step:  1280 | value: 6.84197998046875 | policy -20524.76171875\n",
      "step 1290\n",
      "step:  1290 | value: 7.2946906089782715 | policy -8359.2216796875\n",
      "step:  1300 | value: 6.865091800689697 | policy -1599.4044189453125\n",
      "step:  1310 | value: 6.977664470672607 | policy -3350.458984375\n",
      "step 1320\n",
      "step:  1320 | value: 6.87705135345459 | policy -18109.0390625\n",
      "step:  1330 | value: 6.453563213348389 | policy 141.71835327148438\n",
      "step:  1340 | value: 6.6365509033203125 | policy -34606.1953125\n",
      "step 1350\n",
      "step:  1350 | value: 6.6394524574279785 | policy 8188.4697265625\n",
      "step:  1360 | value: 6.757900238037109 | policy 15928.171875\n",
      "step:  1370 | value: 6.947423458099365 | policy -7407.06640625\n",
      "step 1380\n",
      "step:  1380 | value: 6.174886226654053 | policy -1499.48974609375\n",
      "step:  1390 | value: 7.072103977203369 | policy 8968.421875\n",
      "step:  1400 | value: 6.072988510131836 | policy 4155.3330078125\n",
      "step 1410\n",
      "step:  1410 | value: 6.406890392303467 | policy -11507.212890625\n",
      "step:  1420 | value: 6.471024036407471 | policy -40231.5078125\n",
      "step:  1430 | value: 5.695437431335449 | policy -14145.302734375\n",
      "step 1440\n",
      "step:  1440 | value: 7.121765613555908 | policy -31261.26953125\n",
      "step:  1450 | value: 6.0298285484313965 | policy 19137.662109375\n",
      "step:  1460 | value: 6.007749557495117 | policy 39107.0390625\n",
      "step 1470\n",
      "step:  1470 | value: 5.964322090148926 | policy -6253.576171875\n",
      "step:  1480 | value: 6.802305221557617 | policy 4608.21484375\n",
      "step:  1490 | value: 6.964797019958496 | policy 19534.640625\n",
      "step 1500\n",
      "step:  1500 | value: 5.837649822235107 | policy -5920.9423828125\n",
      "step:  1510 | value: 5.602296829223633 | policy 12382.576171875\n",
      "step:  1520 | value: 6.437741756439209 | policy -24376.11328125\n",
      "step 1530\n",
      "step:  1530 | value: 6.558207988739014 | policy 9875.67578125\n",
      "step:  1540 | value: 5.868684768676758 | policy 3094.2255859375\n",
      "step:  1550 | value: 6.051565170288086 | policy -22379.0\n",
      "step 1560\n",
      "step:  1560 | value: 5.31500244140625 | policy 9740.390625\n",
      "step:  1570 | value: 6.062208652496338 | policy -15447.375\n",
      "step:  1580 | value: 5.980325222015381 | policy -18589.12109375\n",
      "step 1590\n",
      "step:  1590 | value: 5.6446533203125 | policy -3096.592041015625\n",
      "step:  1600 | value: 5.466494083404541 | policy -4721.6376953125\n",
      "step:  1610 | value: 5.707174301147461 | policy 6686.07373046875\n",
      "step 1620\n",
      "step:  1620 | value: 5.465553283691406 | policy 10815.611328125\n",
      "step:  1630 | value: 5.141985893249512 | policy 9681.759765625\n",
      "step:  1640 | value: 5.563714981079102 | policy -36618.765625\n",
      "step 1650\n",
      "step:  1650 | value: 5.07942533493042 | policy 11497.8515625\n",
      "step:  1660 | value: 5.272242069244385 | policy 368.93975830078125\n",
      "step:  1670 | value: 4.971595287322998 | policy -5046.9150390625\n",
      "step 1680\n",
      "step:  1680 | value: 5.595553874969482 | policy -12095.7177734375\n",
      "step:  1690 | value: 5.185856342315674 | policy -21531.5078125\n",
      "step:  1700 | value: 4.776392459869385 | policy -1374.46728515625\n",
      "step 1710\n",
      "step:  1710 | value: 4.937819480895996 | policy -9231.607421875\n",
      "step:  1720 | value: 5.2942795753479 | policy 4758.59326171875\n",
      "step:  1730 | value: 5.4463582038879395 | policy -5550.1240234375\n",
      "step 1740\n",
      "step:  1740 | value: 5.526021957397461 | policy 15229.0703125\n",
      "step:  1750 | value: 4.24462366104126 | policy -4364.00146484375\n",
      "step:  1760 | value: 5.262565612792969 | policy -21262.697265625\n",
      "step 1770\n",
      "step:  1770 | value: 5.442397594451904 | policy 7633.7890625\n",
      "step:  1780 | value: 4.564426422119141 | policy 11164.43359375\n",
      "step:  1790 | value: 4.7830705642700195 | policy 2335.89306640625\n",
      "step 1800\n",
      "step:  1800 | value: 4.823326587677002 | policy 16106.05078125\n",
      "step:  1810 | value: 4.613986015319824 | policy 12934.45703125\n",
      "step:  1820 | value: 4.814673900604248 | policy -355.1871337890625\n",
      "step 1830\n",
      "step:  1830 | value: 4.743912220001221 | policy 34890.40625\n",
      "step:  1840 | value: 5.020115852355957 | policy 7120.85107421875\n",
      "step:  1850 | value: 4.931685447692871 | policy -8051.76171875\n",
      "step 1860\n",
      "step:  1860 | value: 4.68657922744751 | policy -12768.7236328125\n",
      "step:  1870 | value: 5.0781145095825195 | policy -6923.48046875\n",
      "step:  1880 | value: 4.99491548538208 | policy -4056.6337890625\n",
      "step 1890\n",
      "step:  1890 | value: 4.692145347595215 | policy -10823.4130859375\n",
      "step:  1900 | value: 4.683562755584717 | policy -12503.9375\n",
      "step:  1910 | value: 4.068270206451416 | policy 7205.05615234375\n",
      "step 1920\n",
      "step:  1920 | value: 4.370466232299805 | policy -31083.37109375\n",
      "step:  1930 | value: 5.081241607666016 | policy -5472.52783203125\n",
      "step:  1940 | value: 4.432181358337402 | policy 5702.646484375\n",
      "step 1950\n",
      "step:  1950 | value: 4.49075984954834 | policy -4972.80078125\n",
      "step:  1960 | value: 4.421352386474609 | policy -16112.6982421875\n",
      "step:  1970 | value: 4.427987575531006 | policy 28681.888671875\n",
      "step 1980\n",
      "step:  1980 | value: 4.1523237228393555 | policy 2795.4345703125\n",
      "step:  1990 | value: 4.33373498916626 | policy -13412.66015625\n",
      "step:  2000 | value: 4.426019191741943 | policy 8143.15625\n",
      "step 2010\n",
      "step:  2010 | value: 3.570544719696045 | policy 21542.2578125\n",
      "step:  2020 | value: 4.335353851318359 | policy 43357.765625\n",
      "step:  2030 | value: 4.477479934692383 | policy -1674.5416259765625\n",
      "step 2040\n",
      "step:  2040 | value: 4.223843574523926 | policy -19380.50390625\n",
      "step:  2050 | value: 3.812244176864624 | policy 5623.35546875\n",
      "step:  2060 | value: 3.884823799133301 | policy -8240.482421875\n",
      "step 2070\n",
      "step:  2070 | value: 4.180986404418945 | policy -7803.875\n",
      "step:  2080 | value: 4.119604110717773 | policy -2020.6690673828125\n",
      "step:  2090 | value: 4.020190715789795 | policy -826.54833984375\n",
      "step 2100\n",
      "step:  2100 | value: 4.076271057128906 | policy 19631.37890625\n",
      "step:  2110 | value: 4.247485160827637 | policy -4907.06787109375\n",
      "step:  2120 | value: 4.482578277587891 | policy -18172.998046875\n",
      "step 2130\n",
      "step:  2130 | value: 3.995675802230835 | policy 10751.72265625\n",
      "step:  2140 | value: 3.58332896232605 | policy -13598.50390625\n",
      "step:  2150 | value: 3.8531177043914795 | policy 19082.8203125\n",
      "step 2160\n",
      "step:  2160 | value: 3.656510591506958 | policy 18965.7734375\n",
      "step:  2170 | value: 3.915328025817871 | policy -9290.224609375\n",
      "step:  2180 | value: 3.913113594055176 | policy 42473.4609375\n",
      "step 2190\n",
      "step:  2190 | value: 3.810884952545166 | policy 4075.78271484375\n",
      "step:  2200 | value: 3.731046438217163 | policy -15726.9599609375\n",
      "step:  2210 | value: 3.646819591522217 | policy -10102.61328125\n",
      "step 2220\n",
      "step:  2220 | value: 4.0567498207092285 | policy 3466.03125\n",
      "step:  2230 | value: 4.03035306930542 | policy 44483.96875\n",
      "step:  2240 | value: 3.528141498565674 | policy -13128.505859375\n",
      "step 2250\n",
      "step:  2250 | value: 3.4176411628723145 | policy -4318.88427734375\n",
      "step:  2260 | value: 3.8934295177459717 | policy 18916.12890625\n",
      "step:  2270 | value: 3.884577512741089 | policy -4163.078125\n",
      "step 2280\n",
      "step:  2280 | value: 3.858436107635498 | policy 13099.9375\n",
      "step:  2290 | value: 3.5770959854125977 | policy -32491.892578125\n",
      "step:  2300 | value: 3.587597608566284 | policy 11066.671875\n",
      "step 2310\n",
      "step:  2310 | value: 3.549748420715332 | policy 19159.10546875\n",
      "step:  2320 | value: 3.6411571502685547 | policy -306.1877746582031\n",
      "step:  2330 | value: 3.357886791229248 | policy 670.7208862304688\n",
      "step 2340\n",
      "step:  2340 | value: 3.557107448577881 | policy -28304.544921875\n",
      "step:  2350 | value: 3.610440969467163 | policy -1054.959228515625\n",
      "step:  2360 | value: 3.125662088394165 | policy 489.0777282714844\n",
      "step 2370\n",
      "step:  2370 | value: 3.766939401626587 | policy 19468.66796875\n",
      "step:  2380 | value: 3.2917990684509277 | policy -18070.1875\n",
      "step:  2390 | value: 3.180529832839966 | policy 11767.0234375\n",
      "step 2400\n",
      "step:  2400 | value: 3.4962615966796875 | policy -23367.45703125\n",
      "step:  2410 | value: 3.4211480617523193 | policy -21431.181640625\n",
      "step:  2420 | value: 3.534168004989624 | policy 16236.15625\n",
      "step 2430\n",
      "step:  2430 | value: 3.263183116912842 | policy 13292.732421875\n",
      "step:  2440 | value: 3.4057083129882812 | policy -9527.75\n",
      "step:  2450 | value: 3.205739736557007 | policy -12882.3671875\n",
      "step 2460\n",
      "step:  2460 | value: 3.447690010070801 | policy 12918.419921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  2470 | value: 3.309230089187622 | policy 23788.26953125\n",
      "step:  2480 | value: 3.086838722229004 | policy -13578.12890625\n",
      "step 2490\n",
      "step:  2490 | value: 2.963146448135376 | policy 10986.34765625\n",
      "step:  2500 | value: 3.2121074199676514 | policy 39080.453125\n",
      "step:  2510 | value: 3.084400177001953 | policy 11100.427734375\n",
      "step 2520\n",
      "step:  2520 | value: 3.132249355316162 | policy 5319.16259765625\n",
      "step:  2530 | value: 3.0717625617980957 | policy 1523.568359375\n",
      "step:  2540 | value: 3.2605490684509277 | policy -16160.1259765625\n",
      "step 2550\n",
      "step:  2550 | value: 2.91607403755188 | policy -16951.5546875\n",
      "step:  2560 | value: 3.175579786300659 | policy -19928.798828125\n",
      "step:  2570 | value: 3.106559991836548 | policy -30896.44921875\n",
      "step 2580\n",
      "step:  2580 | value: 3.1524198055267334 | policy 1927.8369140625\n",
      "step:  2590 | value: 2.9477086067199707 | policy 17286.58203125\n",
      "step:  2600 | value: 2.92819881439209 | policy 6001.7998046875\n",
      "step 2610\n",
      "step:  2610 | value: 2.9511878490448 | policy 6979.46337890625\n",
      "step:  2620 | value: 3.102651357650757 | policy -25354.30078125\n",
      "step:  2630 | value: 3.1956260204315186 | policy -2866.073486328125\n",
      "step 2640\n",
      "step:  2640 | value: 2.558680295944214 | policy 12288.0546875\n",
      "step:  2650 | value: 2.8713862895965576 | policy -10213.16796875\n",
      "step:  2660 | value: 2.8406739234924316 | policy -8630.6142578125\n",
      "step 2670\n",
      "step:  2670 | value: 3.115504264831543 | policy -14946.4130859375\n",
      "step:  2680 | value: 2.8010520935058594 | policy 16670.375\n",
      "step:  2690 | value: 2.832205295562744 | policy 17478.919921875\n",
      "step 2700\n",
      "step:  2700 | value: 2.56107759475708 | policy -4125.43115234375\n",
      "step:  2710 | value: 2.5544161796569824 | policy 28466.45703125\n",
      "step:  2720 | value: 3.1298859119415283 | policy -1761.0926513671875\n",
      "step 2730\n",
      "step:  2730 | value: 2.8894596099853516 | policy 4215.89306640625\n",
      "step:  2740 | value: 2.86612868309021 | policy 9058.53515625\n",
      "step:  2750 | value: 2.7237746715545654 | policy -15169.8740234375\n",
      "step 2760\n",
      "step:  2760 | value: 2.733452558517456 | policy -9916.3310546875\n",
      "step:  2770 | value: 2.686215400695801 | policy 7926.1298828125\n",
      "step:  2780 | value: 2.743292808532715 | policy 582.947265625\n",
      "step 2790\n",
      "step:  2790 | value: 3.0214598178863525 | policy 6717.06884765625\n",
      "step:  2800 | value: 2.8007683753967285 | policy 17105.126953125\n",
      "step:  2810 | value: 2.7588021755218506 | policy -21815.041015625\n",
      "step 2820\n",
      "step:  2820 | value: 2.698981523513794 | policy -8560.416015625\n",
      "step:  2830 | value: 2.6485650539398193 | policy 1934.79150390625\n",
      "step:  2840 | value: 2.678769111633301 | policy -54337.84765625\n",
      "step 2850\n",
      "step:  2850 | value: 2.7173218727111816 | policy -9734.876953125\n",
      "step:  2860 | value: 2.7231273651123047 | policy -10548.5166015625\n",
      "step:  2870 | value: 2.970721960067749 | policy -15568.244140625\n",
      "step 2880\n",
      "step:  2880 | value: 2.533747911453247 | policy 7946.04833984375\n",
      "step:  2890 | value: 2.574685573577881 | policy -24507.31640625\n",
      "step:  2900 | value: 2.461733341217041 | policy -17978.52734375\n",
      "step 2910\n",
      "step:  2910 | value: 2.3152098655700684 | policy -4466.2568359375\n",
      "step:  2920 | value: 2.4665987491607666 | policy 7131.3154296875\n",
      "step:  2930 | value: 2.24358868598938 | policy 21297.5\n",
      "step 2940\n",
      "step:  2940 | value: 2.305788993835449 | policy -3583.51513671875\n",
      "step:  2950 | value: 2.310661554336548 | policy 6982.43896484375\n",
      "step:  2960 | value: 2.6949710845947266 | policy 17912.3828125\n",
      "step 2970\n",
      "step:  2970 | value: 2.5503525733947754 | policy -2790.9853515625\n",
      "step:  2980 | value: 2.6162445545196533 | policy 11533.181640625\n",
      "step:  2990 | value: 2.345080852508545 | policy -2534.93701171875\n",
      "step 3000\n",
      "step:  3000 | value: 2.539125442504883 | policy 3303.63232421875\n",
      "step:  3010 | value: 2.254065752029419 | policy 14091.982421875\n",
      "step:  3020 | value: 2.303130626678467 | policy 15530.6708984375\n",
      "step 3030\n",
      "step:  3030 | value: 2.608792543411255 | policy 2486.696533203125\n",
      "step:  3040 | value: 2.2353224754333496 | policy -2270.943603515625\n",
      "step:  3050 | value: 2.59110164642334 | policy 10704.97265625\n",
      "step 3060\n",
      "step:  3060 | value: 2.4967355728149414 | policy 3169.4169921875\n",
      "step:  3070 | value: 2.413804054260254 | policy 16752.43359375\n",
      "step:  3080 | value: 2.2521963119506836 | policy -16025.0390625\n",
      "step 3090\n",
      "step:  3090 | value: 2.4604318141937256 | policy 24386.45703125\n",
      "step:  3100 | value: 2.4237098693847656 | policy -8873.2392578125\n",
      "step:  3110 | value: 2.116365909576416 | policy -11783.7802734375\n",
      "step 3120\n",
      "step:  3120 | value: 2.560959815979004 | policy 14898.75\n",
      "step:  3130 | value: 2.2671144008636475 | policy -18691.75\n",
      "step:  3140 | value: 2.415348529815674 | policy -12162.453125\n",
      "step 3150\n",
      "step:  3150 | value: 2.199389934539795 | policy 1748.71337890625\n",
      "step:  3160 | value: 2.2179887294769287 | policy -1728.174560546875\n",
      "step:  3170 | value: 2.381166934967041 | policy 25055.0390625\n",
      "step 3180\n",
      "step:  3180 | value: 2.347168207168579 | policy -20494.4140625\n",
      "step:  3190 | value: 2.1416079998016357 | policy -9595.12109375\n",
      "step:  3200 | value: 2.230300188064575 | policy 10872.453125\n",
      "step 3210\n",
      "step:  3210 | value: 2.2617392539978027 | policy -6344.22314453125\n",
      "step:  3220 | value: 2.2526466846466064 | policy -12106.41796875\n",
      "step:  3230 | value: 2.0948314666748047 | policy -8646.4560546875\n",
      "step 3240\n",
      "step:  3240 | value: 2.1696135997772217 | policy 22086.19140625\n",
      "step:  3250 | value: 2.03834867477417 | policy 5620.62646484375\n",
      "step:  3260 | value: 1.8727604150772095 | policy 20335.326171875\n",
      "step 3270\n",
      "step:  3270 | value: 2.377664089202881 | policy -11800.7294921875\n",
      "step:  3280 | value: 2.277233600616455 | policy -3439.2978515625\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in tqdm(env.train_dataloader):\n",
    "        loss = reinforce_update(batch, params, nets, optimizer,\n",
    "                     writer=writer,\n",
    "                     device=device,\n",
    "                     debug=debug, learn=True, step=step)\n",
    "        if loss:\n",
    "            plotter.log_losses(loss)\n",
    "        step += 1\n",
    "        if step % plot_every == 0:\n",
    "            # clear_output(True)\n",
    "            print('step', step)\n",
    "            #test_loss = run_tests()\n",
    "            #plotter.log_losses(test_loss, test=True)\n",
    "            #plotter.plot_loss()\n",
    "        #if step > 1000:\n",
    "        #    pass\n",
    "        #    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
