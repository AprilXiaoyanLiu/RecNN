{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# == recnn ==\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import recnn\n",
    "\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "# ---\n",
    "frame_size = 10\n",
    "batch_size = 10\n",
    "n_epochs   = 100\n",
    "plot_every = 30\n",
    "step       = 0\n",
    "num_items    = 5000 # n items to recommend. Can be adjusted for your vram \n",
    "# --- \n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='grade3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will drop low freq items because it doesnt fit into my videocard vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataset(df, key_to_id, frame_size, env, sort_users=False):\n",
    "    \n",
    "    global num_items\n",
    "    \n",
    "    value_counts = df['movieId'].value_counts() \n",
    "    print('counted!')\n",
    "    \n",
    "    num_items = 5000\n",
    "    to_remove = df['movieId'].value_counts().sort_values()[:-num_items].index\n",
    "    to_keep = df['movieId'].value_counts().sort_values()[-num_items:].index\n",
    "    to_remove_indices = df[df['movieId'].isin(to_remove)].index\n",
    "    num_removed = len(to_remove)\n",
    "    \n",
    "    df.drop(to_remove_indices, inplace=True)\n",
    "    print('dropped!')\n",
    "    \n",
    "    print('before', env.embeddings.size(), len(env.movie_embeddings_key_dict))\n",
    "    for i in list(env.movie_embeddings_key_dict.keys()):\n",
    "        if i not in to_keep:\n",
    "            del env.movie_embeddings_key_dict[i]\n",
    "        \n",
    "    env.embeddings, env.key_to_id, env.id_to_key = recnn.data.utils.make_items_tensor(env.movie_embeddings_key_dict)\n",
    "    \n",
    "    print('after', env.embeddings.size(), len(env.movie_embeddings_key_dict))\n",
    "    print('embeddings automatically updated')\n",
    "    print('action space is reduced to {} - {} = {}'.format(num_items + num_removed, num_removed,\n",
    "                                                           num_items))\n",
    "    \n",
    "    return recnn.data.prepare_dataset(df, env.key_to_id, frame_size, sort_users=sort_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_contstate_discaction(batch, item_embeddings_tensor, frame_size, num_items, *args, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Embed Batch: continuous state discrete action\n",
    "    \"\"\"\n",
    "    \n",
    "    from recnn.data.utils import get_irsu\n",
    "    \n",
    "    items_t, ratings_t, sizes_t, users_t = get_irsu(batch)\n",
    "    items_emb = item_embeddings_tensor[items_t.long()]\n",
    "    b_size = ratings_t.size(0)\n",
    "\n",
    "    items = items_emb[:, :-1, :].view(b_size, -1)\n",
    "    next_items = items_emb[:, 1:, :].view(b_size, -1)\n",
    "    ratings = ratings_t[:, :-1]\n",
    "    next_ratings = ratings_t[:, 1:]\n",
    "\n",
    "    state = torch.cat([items, ratings], 1)\n",
    "    next_state = torch.cat([next_items, next_ratings], 1)\n",
    "    action = items_t[:, -1]\n",
    "    reward = ratings_t[:, -1]\n",
    "\n",
    "    done = torch.zeros(b_size)\n",
    "    done[torch.cumsum(sizes_t - frame_size, dim=0) - 1] = 1\n",
    "    \n",
    "    one_hot_action = torch.zeros(action.size(0), num_items)\n",
    "    one_hot_action.scatter_(1, action.view(-1,1), 1)\n",
    "\n",
    "    batch = {'state': state, 'action': one_hot_action, 'reward': reward, 'next_state': next_state, 'done': done,\n",
    "             'meta': {'users': users_t, 'sizes': sizes_t}}\n",
    "    return batch\n",
    "\n",
    "def embed_batch(batch, item_embeddings_tensor, *args, **kwargs):\n",
    "    return batch_contstate_discaction(batch, item_embeddings_tensor, frame_size=frame_size, num_items=num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counted!\n",
      "dropped!\n",
      "before torch.Size([27278, 128]) 27278\n",
      "after torch.Size([5000, 128]) 5000\n",
      "embeddings automatically updated\n",
      "action space is reduced to 26744 - 21744 = 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7114a599f24df8a8b10b0086793380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18946308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120ff100ce3a4a5a919de014f368be6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18946308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cb8530bf254284bdd653b2a447f50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=138493), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# embeddgings: https://drive.google.com/open?id=1EQ_zXBR3DKpmJR3jBgLvt-xoOvArGMsL\n",
    "env = recnn.data.env.FrameEnv('../../data/embeddings/ml20_pca128.pkl',\n",
    "                              '../../data/ml-20m/ratings.csv', frame_size, batch_size,\n",
    "                              embed_batch=embed_batch, prepare_dataset=prepare_dataset,\n",
    "                              num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePolicy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, num_actions):\n",
    "        super(DiscretePolicy, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        action_scores = self.linear2(x)\n",
    "        return F.softmax(action_scores)\n",
    "    \n",
    "    \n",
    "def select_action(policy, state):\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because I do not have a dynamic environment, I also will include a critic. If you have a real non static environment, you can do w/o citic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Critic(\n",
       "  (drop_layer): Dropout(p=0.5, inplace=False)\n",
       "  (linear1): Linear(in_features=6290, out_features=2048, bias=True)\n",
       "  (linear2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (linear3): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_net = recnn.nn.Critic(1290, num_items, 2048, 54e-2).to(cuda)\n",
    "target_value_net = recnn.nn.Critic(1290, num_items, 2048, 54e-2).to(cuda)\n",
    "\n",
    "policy_net = DiscretePolicy(2048, 1290, num_items).to(cuda)\n",
    "target_policy_net = DiscretePolicy(2048, 1290, num_items).to(cuda)\n",
    "\n",
    "policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "value_optimizer = torch.optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "target_policy_net.eval()\n",
    "target_value_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_update(batch, learn=True):\n",
    "    \n",
    "    state, action, reward, next_state, done = recnn.data.get_base_batch(batch)\n",
    "    \n",
    "    # Value Learning\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_action = target_policy_net(next_state)\n",
    "        target_value   = target_value_net(next_state, next_action.detach())\n",
    "        expected_value = reward + (1.0 - done) * 0.99 * target_value\n",
    "        expected_value = torch.clamp(expected_value, -10, 10)\n",
    "\n",
    "    value = value_net(state, action)\n",
    "    value_loss = torch.pow(value - expected_value.detach(), 2).mean()\n",
    "    \n",
    "    if learn:\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "        \n",
    "    return value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE(policy, optimizer):\n",
    "    R = torch.tensor([0]).to(cuda)\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + 0.99 * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 0.0001)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "\n",
    "        policy_loss.append(-log_prob * R)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "    \n",
    "    return policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(batch, step):\n",
    "    state, action, reward, next_state, done = recnn.data.get_base_batch(batch)\n",
    "    \n",
    "    predicted_action, predicted_probs = select_action(policy_net, state)\n",
    "    reward = value_net(state, predicted_probs).detach()\n",
    "    policy_net.rewards.append(reward.mean())\n",
    "    \n",
    "    value_loss = td_update(batch)\n",
    "    \n",
    "    if step % 10 == 0 and step > 0:\n",
    "        policy_loss = REINFORCE(policy_net, policy_optimizer)\n",
    "        del policy_net.rewards[:]\n",
    "        del policy_net.saved_log_probs[:]\n",
    "        print('step: ', step, '| value:', value_loss.item(), '| policy', policy_loss.item())\n",
    "    \n",
    "    recnn.utils.soft_update(value_net, target_value_net, soft_tau=0.001)\n",
    "    recnn.utils.soft_update(policy_net, target_policy_net, soft_tau=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7042142061b435985cefde01ff59dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  10 | value: 3.483903408050537 | policy -661.602294921875\n",
      "step:  20 | value: 5.725956916809082 | policy -5057.4697265625\n",
      "step:  30 | value: 3.3426642417907715 | policy 1540.8583984375\n",
      "step:  40 | value: 4.142497539520264 | policy -324.4228515625\n",
      "step:  50 | value: 4.274340629577637 | policy -2936.5673828125\n",
      "step:  60 | value: 3.9849038124084473 | policy -1814.1396484375\n",
      "step:  70 | value: 4.143095970153809 | policy -2763.79541015625\n",
      "step:  80 | value: 4.20749568939209 | policy 1278.69873046875\n",
      "step:  90 | value: 3.7009503841400146 | policy -2046.633544921875\n",
      "step:  100 | value: 3.5018255710601807 | policy 2278.51318359375\n",
      "step:  110 | value: 3.5440173149108887 | policy -1018.57861328125\n",
      "step:  120 | value: 3.3337490558624268 | policy 1508.5556640625\n",
      "step:  130 | value: 3.275139093399048 | policy -5600.7783203125\n",
      "step:  140 | value: 4.0005574226379395 | policy 717.43359375\n",
      "step:  150 | value: 3.958998680114746 | policy 2416.165771484375\n",
      "step:  160 | value: 3.454111099243164 | policy 654.7803955078125\n",
      "step:  170 | value: 4.8537445068359375 | policy -3833.34228515625\n",
      "step:  180 | value: 3.2883660793304443 | policy -75.25181579589844\n",
      "step:  190 | value: 3.123929500579834 | policy 1576.4996337890625\n",
      "step:  200 | value: 2.8015923500061035 | policy 1348.3916015625\n",
      "step:  210 | value: 3.444328784942627 | policy -1202.150146484375\n",
      "step:  220 | value: 3.097540855407715 | policy 1288.968017578125\n",
      "step:  230 | value: 3.730706214904785 | policy 112.03036499023438\n",
      "step:  240 | value: 3.8075404167175293 | policy 1461.83642578125\n",
      "step:  250 | value: 2.9934191703796387 | policy 1325.400634765625\n",
      "step:  260 | value: 2.408552408218384 | policy -131.5269012451172\n",
      "step:  270 | value: 4.385932922363281 | policy 1070.88525390625\n",
      "step:  280 | value: 4.319593906402588 | policy -1347.2747802734375\n",
      "step:  290 | value: 4.139045715332031 | policy 2348.87841796875\n",
      "step:  300 | value: 3.3487730026245117 | policy -2795.9765625\n",
      "step:  310 | value: 3.424750566482544 | policy -3130.65185546875\n",
      "step:  320 | value: 1.7634451389312744 | policy -3116.6318359375\n",
      "step:  330 | value: 3.471107244491577 | policy -1981.552001953125\n",
      "step:  340 | value: 3.589639902114868 | policy -673.9940185546875\n",
      "step:  350 | value: 2.5173306465148926 | policy 336.28265380859375\n",
      "step:  360 | value: 3.4804086685180664 | policy 227.77008056640625\n",
      "step:  370 | value: 3.1369845867156982 | policy -1079.1727294921875\n",
      "step:  380 | value: 3.4591660499572754 | policy -1965.169921875\n",
      "step:  390 | value: 3.700962543487549 | policy -1435.943115234375\n",
      "step:  400 | value: 4.226112365722656 | policy 106.01040649414062\n",
      "step:  410 | value: 3.6446096897125244 | policy -1442.1796875\n",
      "step:  420 | value: 2.252779245376587 | policy 75.6151351928711\n",
      "step:  430 | value: 2.9015159606933594 | policy 340.67584228515625\n",
      "step:  440 | value: 4.954002380371094 | policy -186.54055786132812\n",
      "step:  450 | value: 3.6059629917144775 | policy 197.96163940429688\n",
      "step:  460 | value: 3.691903829574585 | policy 57.38982009887695\n",
      "step:  470 | value: 3.25134015083313 | policy -219.11849975585938\n",
      "step:  480 | value: 2.1613287925720215 | policy -15.913848876953125\n",
      "step:  490 | value: 3.049499750137329 | policy -523.1387939453125\n",
      "step:  500 | value: 3.639073133468628 | policy 64.86707305908203\n",
      "step:  510 | value: 3.0447049140930176 | policy 220.42774963378906\n",
      "step:  520 | value: 3.6294023990631104 | policy 75.93373107910156\n",
      "step:  530 | value: 2.7361981868743896 | policy 64.95291137695312\n",
      "step:  540 | value: 2.976778268814087 | policy 92.46623229980469\n",
      "step:  550 | value: 3.249474048614502 | policy 13.690591812133789\n",
      "step:  560 | value: 2.624634265899658 | policy 188.01400756835938\n",
      "step:  570 | value: 2.8260741233825684 | policy -402.58489990234375\n",
      "step:  580 | value: 3.0637857913970947 | policy -98.83721160888672\n",
      "step:  590 | value: 3.645279884338379 | policy 87.30614471435547\n",
      "step:  600 | value: 3.111262559890747 | policy -14.990365982055664\n",
      "step:  610 | value: 3.5943403244018555 | policy -72.60459899902344\n",
      "step:  620 | value: 4.097164630889893 | policy 47.94190979003906\n",
      "step:  630 | value: 3.6999988555908203 | policy -53.01466369628906\n",
      "step:  640 | value: 3.4853055477142334 | policy -107.95777130126953\n",
      "step:  650 | value: 2.8207967281341553 | policy 107.97798156738281\n",
      "step:  660 | value: 4.73027229309082 | policy -27.22995376586914\n",
      "step:  670 | value: 3.7788584232330322 | policy 78.4549560546875\n",
      "step:  680 | value: 3.459317207336426 | policy -14.942771911621094\n",
      "step:  690 | value: 4.5445451736450195 | policy -69.34186553955078\n",
      "step:  700 | value: 3.7508065700531006 | policy 53.9076042175293\n",
      "step:  710 | value: 4.087530612945557 | policy -58.655818939208984\n",
      "step:  720 | value: 6.541646957397461 | policy -40.57725524902344\n",
      "step:  730 | value: 3.9411118030548096 | policy -60.7540283203125\n",
      "step:  740 | value: 4.671401023864746 | policy 29.995210647583008\n",
      "step:  750 | value: 3.589096784591675 | policy -1.6894512176513672\n",
      "step:  760 | value: 3.9032325744628906 | policy -1.614664077758789\n",
      "step:  770 | value: 2.928532361984253 | policy -24.618934631347656\n",
      "step:  780 | value: 3.0395519733428955 | policy -2.2046236991882324\n",
      "step:  790 | value: 3.250626802444458 | policy -17.95273208618164\n",
      "step:  800 | value: 4.444704532623291 | policy 16.680248260498047\n",
      "step:  810 | value: 2.9523258209228516 | policy -0.8986201286315918\n",
      "step:  820 | value: 3.0983338356018066 | policy 15.007010459899902\n",
      "step:  830 | value: 3.5563883781433105 | policy -1.9337941408157349\n",
      "step:  840 | value: 3.013424873352051 | policy -31.159738540649414\n",
      "step:  850 | value: 3.9589362144470215 | policy 2.8363876342773438\n",
      "step:  860 | value: 3.80635142326355 | policy 2.063100814819336\n",
      "step:  870 | value: 3.1154613494873047 | policy 2.811182975769043\n",
      "step:  880 | value: 3.375886917114258 | policy 6.8645429611206055\n",
      "step:  890 | value: 3.551374912261963 | policy -0.017316102981567383\n",
      "step:  900 | value: 2.9259448051452637 | policy 28.84357452392578\n",
      "step:  910 | value: 2.152890205383301 | policy 2.9263429641723633\n",
      "step:  920 | value: 2.888267755508423 | policy 5.886483192443848\n",
      "step:  930 | value: 3.975977897644043 | policy 7.248601913452148\n",
      "step:  940 | value: 3.3042027950286865 | policy -13.617326736450195\n",
      "step:  950 | value: 4.1882758140563965 | policy -0.5042881965637207\n",
      "step:  960 | value: 3.7821764945983887 | policy -9.956693649291992\n",
      "step:  970 | value: 2.508527994155884 | policy -4.994483470916748\n",
      "step:  980 | value: 4.202073574066162 | policy 6.190559387207031\n",
      "step:  990 | value: 2.500190258026123 | policy -4.633466720581055\n",
      "step:  1000 | value: 3.371302366256714 | policy -3.7774925231933594\n",
      "step:  1010 | value: 3.4768259525299072 | policy 14.250896453857422\n",
      "step:  1020 | value: 4.578458786010742 | policy -0.1360321044921875\n",
      "step:  1030 | value: 2.8904290199279785 | policy -4.3303070068359375\n",
      "step:  1040 | value: 3.3269202709198 | policy 9.951383590698242\n",
      "step:  1050 | value: 2.943775177001953 | policy -2.5318479537963867\n",
      "step:  1060 | value: 3.8997716903686523 | policy 1.6298495531082153\n",
      "step:  1070 | value: 2.6205179691314697 | policy 15.946036338806152\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in tqdm(env.train_dataloader):\n",
    "        learn(batch, step)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
